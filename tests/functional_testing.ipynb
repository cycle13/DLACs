{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copyright Netherlands eScience Center <br>\n",
    "** Function     : Predict the Spatial Sea Ice Concentration with BayesConvLSTM at weekly time scale** <br>\n",
    "** Author       : Yang Liu ** <br>\n",
    "** First Built  : 2020.03.02 ** <br>\n",
    "** Last Update  : 2020.03.06 ** <br>\n",
    "** Library      : Pytorth, Numpy, NetCDF4, os, iris, cartopy, dlacs, matplotlib **<br>\n",
    "Description     : This notebook serves to predict the Arctic sea ice using deep learning. The Bayesian Convolutional Long Short Time Memory neural network is used to deal with this spatial-temporal sequence problem. We use Pytorch as the deep learning framework. <br>\n",
    "<br>\n",
    "** Here we predict sea ice concentration with one extra relevant field from either ocean or atmosphere to test the predictor.** <br>\n",
    "\n",
    "Return Values   : Time series and figures <br>\n",
    "\n",
    "The regionalization adopted here follows that of the MASIE (Multisensor Analyzed Sea Ice Extent) product available from the National Snow and Ice Data Center:<br>\n",
    "https://nsidc.org/data/masie/browse_regions<br>\n",
    "It is given by paper J.Walsh et. al., 2019. Benchmark seasonal prediction skill estimates based on regional indices.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import numbers\n",
    "\n",
    "# for data loading\n",
    "import os\n",
    "from netCDF4 import Dataset\n",
    "# for pre-processing and machine learning\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#import scipy\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "#sys.path.append(os.path.join('C:','Users','nosta','ML4Climate','Scripts','DLACs'))\n",
    "#sys.path.append(\"C:\\\\Users\\\\nosta\\\\ML4Climate\\\\Scripts\\\\DLACs\")\n",
    "sys.path.append(\"../\")\n",
    "import dlacs\n",
    "import dlacs.BayesConvLSTM\n",
    "import dlacs.preprocess\n",
    "import dlacs.function\n",
    "\n",
    "# for visualization\n",
    "import dlacs.visual\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import iris # also helps with regriding\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# ignore all the DeprecationWarnings by pytorch\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing device is Dell Inspirion 5680 with Intel Core i7-8700 x64 CPU and Nvidia GTX 1060 6GB GPU.<br>\n",
    "Here is a benchmark about cpu v.s. gtx 1060 <br>\n",
    "https://www.analyticsindiamag.com/deep-learning-tensorflow-benchmark-intel-i5-4210u-vs-geforce-nvidia-1060-6gb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "constant = {'g' : 9.80616,      # gravititional acceleration [m / s2]\n",
    "            'R' : 6371009,      # radius of the earth [m]\n",
    "            'cp': 1004.64,      # heat capacity of air [J/(Kg*K)]\n",
    "            'Lv': 2500000,      # Latent heat of vaporization [J/Kg]\n",
    "            'R_dry' : 286.9,    # gas constant of dry air [J/(kg*K)]\n",
    "            'R_vap' : 461.5,    # gas constant for water vapour [J/(kg*K)]\n",
    "            'rho' : 1026,       # sea water density [kg/m3]\n",
    "            }\n",
    "\n",
    "################################################################################# \n",
    "#########                           datapath                             ########\n",
    "#################################################################################\n",
    "# please specify data path\n",
    "datapath_ERAI = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ERA-Interim'\n",
    "#datapath_ERAI = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ERA-Interim'\n",
    "datapath_ORAS4 = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4 = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "datapath_ORAS4_mask = '/home/ESLT0068/WorkFlow/Core_Database_DeepLearn/ORAS4'\n",
    "#datapath_ORAS4_mask = 'H:\\\\Creator_Zone\\\\Core_Database_DeepLearn\\\\ORAS4'\n",
    "#datapath_PIOMASS = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/PIOMASS'\n",
    "#datapath_PIOMASS = 'H:\\\\Creator_Zone\\\\Core_Database_AMET_OMET_reanalysis\\\\PIOMASS'\n",
    "#datapath_clim_index = '/home/ESLT0068/WorkFlow/Core_Database_AMET_OMET_reanalysis/Climate_index'\n",
    "#datapath_clim_index = 'F:\\\\PhD_essential\\\\Core_Database_AMET_OMET_reanalysis\\\\Climate_index'\n",
    "output_path = '/home/ESLT0068/NLeSC/Computation_Modeling/ML4Climate/PredictArctic/BayesMaps'\n",
    "#output_path = 'C:\\\\Users\\\\nosta\\\\ML4Climate\\\\PredictArctic\\\\BayesMaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** get the key to the datasets *************************\n",
      "*********************** extract variables *************************\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print ('*********************** get the key to the datasets *************************')\n",
    "    # weekly variables on ERAI grid\n",
    "    dataset_ERAI_fields_sic = Dataset(os.path.join(datapath_ERAI,\n",
    "                                      'sic_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_slp = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       'slp_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_t2m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                       't2m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z500 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z500_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_z850 = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'z850_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_uv10m = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                        'uv10m_weekly_erai_1979_2017.nc'))\n",
    "#     dataset_ERAI_fields_rad = Dataset(os.path.join(datapath_ERAI,\n",
    "#                                         'rad_flux_weekly_erai_1979_2017.nc'))\n",
    "    #dataset_PIOMASS_siv = Dataset(os.path.join(datapath_PIOMASS,\n",
    "    #                             'siv_monthly_PIOMASS_1979_2017.nc'))\n",
    "    # OHC interpolated on ERA-Interim grid\n",
    "    dataset_ORAS4_OHC = Dataset(os.path.join(datapath_ORAS4,\n",
    "                                'ohc_monthly_oras2erai_1978_2017.nc'))\n",
    "#     dataset_index = Dataset(os.path.join(datapath_clim_index,\n",
    "#                             'index_climate_monthly_regress_1950_2017.nc'))\n",
    "    #dataset_ERAI_fields_flux = Dataset(os.path.join(datapath_ERAI_fields,\n",
    "    #                                  'surface_erai_monthly_regress_1979_2017_radiation.nc'))\n",
    "    # mask\n",
    "    dataset_ORAS4_mask = Dataset(os.path.join(datapath_ORAS4_mask, 'mesh_mask.nc'))\n",
    "    print ('*********************** extract variables *************************')\n",
    "    #################################################################################\n",
    "    #########                        data gallery                           #########\n",
    "    #################################################################################\n",
    "    # we use time series from 1979 to 2016 (468 months in total)\n",
    "    # training data: 1979 - 2013\n",
    "    # validation: 2014 - 2016\n",
    "    # variables list:\n",
    "    # SIC (ERA-Interim) / SIV (PIOMASS) / SST (ERA-Interim) / ST (ERA-Interim) / OHC (ORAS4) / AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "    # integrals from spatial fields cover the area from 20N - 90N (4D fields [year, month, lat, lon])\n",
    "    # *************************************************************************************** #\n",
    "    # SIC (ERA-Interim) - benckmark\n",
    "    SIC_ERAI = dataset_ERAI_fields_sic.variables['sic'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "    year_ERAI = dataset_ERAI_fields_sic.variables['year'][:-1]\n",
    "    week_ERAI = dataset_ERAI_fields_sic.variables['week'][:]\n",
    "    latitude_ERAI = dataset_ERAI_fields_sic.variables['latitude'][:]\n",
    "    longitude_ERAI = dataset_ERAI_fields_sic.variables['longitude'][:]\n",
    "    # T2M (ERA-Interim)\n",
    "#     T2M_ERAI = dataset_ERAI_fields_t2m.variables['t2m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_t2m = dataset_ERAI_fields_t2m.variables['year'][:-1]\n",
    "#     week_ERAI_t2m = dataset_ERAI_fields_t2m.variables['week'][:]\n",
    "#     latitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['latitude'][:]\n",
    "#     longitude_ERAI_t2m = dataset_ERAI_fields_t2m.variables['longitude'][:]\n",
    "    # SLP (ERA-Interim)\n",
    "#     SLP_ERAI = dataset_ERAI_fields_slp.variables['slp'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_slp = dataset_ERAI_fields_slp.variables['year'][:-1]\n",
    "#     week_ERAI_slp = dataset_ERAI_fields_slp.variables['week'][:]\n",
    "#     latitude_ERAI_slp = dataset_ERAI_fields_slp.variables['latitude'][:]\n",
    "#     longitude_ERAI_slp = dataset_ERAI_fields_slp.variables['longitude'][:]\n",
    "    # Z500 (ERA-Interim)\n",
    "#     Z500_ERAI = dataset_ERAI_fields_z500.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z500 = dataset_ERAI_fields_z500.variables['year'][:-1]\n",
    "#     week_ERAI_z500 = dataset_ERAI_fields_z500.variables['week'][:]\n",
    "#     latitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['latitude'][:]\n",
    "#     longitude_ERAI_z500 = dataset_ERAI_fields_z500.variables['longitude'][:]\n",
    "    # Z850 (ERA-Interim)\n",
    "#     Z850_ERAI = dataset_ERAI_fields_z850.variables['z'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_z850 = dataset_ERAI_fields_z850.variables['year'][:-1]\n",
    "#     week_ERAI_z850 = dataset_ERAI_fields_z850.variables['week'][:]\n",
    "#     latitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['latitude'][:]\n",
    "#     longitude_ERAI_z850 = dataset_ERAI_fields_z850.variables['longitude'][:]\n",
    "    # UV10M (ERA-Interim)\n",
    "#     U10M_ERAI = dataset_ERAI_fields_uv10m.variables['u10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     V10M_ERAI = dataset_ERAI_fields_uv10m.variables['v10m'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['year'][:-1]\n",
    "#     week_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['week'][:]\n",
    "#     latitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['latitude'][:]\n",
    "#     longitude_ERAI_uv10m = dataset_ERAI_fields_uv10m.variables['longitude'][:]\n",
    "    # SFlux (ERA-Interim)\n",
    "#     SFlux_ERAI = dataset_ERAI_fields_rad.variables['SFlux'][:-1,:,:,:] # 4D fields [year, week, lat, lon]\n",
    "#     year_ERAI_SFlux = dataset_ERAI_fields_rad.variables['year'][:-1]\n",
    "#     week_ERAI_SFlux = dataset_ERAI_fields_rad.variables['week'][:]\n",
    "#     latitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['latitude'][:]\n",
    "#     longitude_ERAI_SFlux = dataset_ERAI_fields_rad.variables['longitude'][:]\n",
    "    #SIV (PIOMASS)\n",
    "    #SIV_PIOMASS = dataset_PIOMASS_siv.variables['SIV'][:-12]\n",
    "    #year_SIV = dataset_PIOMASS_siv.variables['year'][:-1]\n",
    "    # OHC (ORAS4)\n",
    "    # from 1978 - 2017 (for interpolation) / from 90 N upto 40 N\n",
    "    OHC_300_ORAS4 = dataset_ORAS4_OHC.variables['OHC'][:-1,:,:67,:]/1000 # unit Peta Joule\n",
    "    latitude_ORAS4 = dataset_ORAS4_OHC.variables['latitude'][:]\n",
    "    longitude_ORAS4 = dataset_ORAS4_OHC.variables['longitude'][:]\n",
    "    mask_OHC = np.ma.getmask(OHC_300_ORAS4[0,0,:,:])\n",
    "    # AO-NAO-AMO-NINO3.4 (NOAA)\n",
    "#     AO = dataset_index.variables['AO'][348:-1] # from 1979 - 2017\n",
    "#     NAO = dataset_index.variables['NAO'][348:-1]\n",
    "#     NINO = dataset_index.variables['NINO'][348:-1]\n",
    "#     AMO = dataset_index.variables['AMO'][348:-1]\n",
    "#     PDO = dataset_index.variables['PDO'][348:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************** create mask *************************\n",
      "*********************** calc mask *************************\n",
      "*********************** packing *************************\n",
      "******************  calculate extent from spatial fields  *******************\n",
      "================  reshape input data into time series  =================\n",
      "******************  choose the fields from target region  *******************\n",
      "******************  choose the fields from target region  *******************\n",
      "(1824, 24, 56)\n",
      "(1824, 24, 56)\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "[80.5  79.75 79.   78.25 77.5  76.75 76.   75.25 74.5  73.75 73.   72.25\n",
      " 71.5  70.75 70.   69.25 68.5  67.75 67.   66.25 65.5  64.75 64.   63.25]\n",
      "[18.   18.75 19.5  20.25 21.   21.75 22.5  23.25 24.   24.75 25.5  26.25\n",
      " 27.   27.75 28.5  29.25 30.   30.75 31.5  32.25 33.   33.75 34.5  35.25\n",
      " 36.   36.75 37.5  38.25 39.   39.75 40.5  41.25 42.   42.75 43.5  44.25\n",
      " 45.   45.75 46.5  47.25 48.   48.75 49.5  50.25 51.   51.75 52.5  53.25\n",
      " 54.   54.75 55.5  56.25 57.   57.75 58.5  59.25]\n",
      "*******************  pre-processing  *********************\n",
      "=========================   normalize data   ===========================\n",
      "================  save the normalizing factor  =================\n",
      "1565.2049481856002 km2\n",
      "0.0 km2\n",
      "====================    A series of time (index)    ====================\n",
      "===================  artificial data for evaluation ====================\n"
     ]
    }
   ],
   "source": [
    "    #################################################################################\n",
    "    ###########                 global land-sea mask                      ###########\n",
    "    #################################################################################\n",
    "    sea_ice_mask_global = np.ones((len(latitude_ERAI),len(longitude_ERAI)),dtype=float)\n",
    "    sea_ice_mask_global[SIC_ERAI[0,0,:,:]==-1] = 0\n",
    "    #################################################################################\n",
    "    ###########                regionalization sea mask                   ###########\n",
    "    #################################################################################\n",
    "    print ('*********************** create mask *************************')\n",
    "    # W:-156 E:-124 N:80 S:67\n",
    "    mask_Beaufort = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:-156 N:80 S:66\n",
    "    mask_Chukchi = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:146 E:180 N:80 S:67\n",
    "    mask_EastSiberian = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:100 E:146 N:80 S:67\n",
    "    mask_Laptev = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:60 E:100 N:80 S:67\n",
    "    mask_Kara = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:18 E:60 N:80 S:64\n",
    "    mask_Barents = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-44 E:18 N:80 S:55\n",
    "    mask_Greenland = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    # W:-180 E:180 N:90 S:80\n",
    "    mask_CenArctic = np.zeros((len(latitude_ERAI),len(longitude_ERAI)),dtype=int)\n",
    "    print ('*********************** calc mask *************************')\n",
    "    mask_Beaufort[13:31,32:76] = 1\n",
    "\n",
    "    mask_Chukchi[13:32,0:32] = 1\n",
    "    mask_Chukchi[13:32,-1] = 1\n",
    "\n",
    "    mask_EastSiberian[13:31,434:479] = 1\n",
    "\n",
    "    mask_Laptev[13:31,374:434] = 1\n",
    "\n",
    "    mask_Kara[13:31,320:374] = 1\n",
    "\n",
    "    mask_Barents[13:36,264:320] = 1\n",
    "\n",
    "    mask_Greenland[13:47,179:264] = 1\n",
    "    mask_Greenland[26:47,240:264] = 0\n",
    "\n",
    "    mask_CenArctic[:13,:] = 1\n",
    "    print ('*********************** packing *************************')\n",
    "    mask_dict = {'Beaufort': mask_Beaufort[:,:],\n",
    "                 'Chukchi': mask_Chukchi[:,:],\n",
    "                 'EastSiberian': mask_EastSiberian[:,:],\n",
    "                 'Laptev': mask_Laptev[:,:],\n",
    "                 'Kara': mask_Kara[:,:],\n",
    "                 'Barents': mask_Barents[:,:],\n",
    "                 'Greenland': mask_Greenland[:,:],\n",
    "                 'CenArctic': mask_CenArctic[:,:]}\n",
    "    seas_namelist = ['Beaufort','Chukchi','EastSiberian','Laptev',\n",
    "                     'Kara', 'Barents', 'Greenland','CenArctic']\n",
    "    #################################################################################\n",
    "    ########                  temporal interpolation matrix                  ########\n",
    "    #################################################################################\n",
    "    # interpolate from monthly to weekly\n",
    "    # original monthly data will be taken as the last week of the month\n",
    "    OHC_300_ORAS4_weekly_series = np.zeros(SIC_ERAI.reshape(len(year_ERAI)*48,len(latitude_ERAI),len(longitude_ERAI)).shape,\n",
    "                                           dtype=float)\n",
    "    OHC_300_ORAS4_series= dlacs.preprocess.operator.unfold(OHC_300_ORAS4)\n",
    "    # calculate the difference between two months\n",
    "    OHC_300_ORAS4_deviation_series = (OHC_300_ORAS4_series[1:,:,:] - OHC_300_ORAS4_series[:-1,:,:]) / 4\n",
    "    for i in np.arange(4):\n",
    "        OHC_300_ORAS4_weekly_series[3-i::4,:,:] = OHC_300_ORAS4_series[12:,:,:] - i * OHC_300_ORAS4_deviation_series[11:,:,:]\n",
    "\n",
    "    print ('******************  calculate extent from spatial fields  *******************')\n",
    "    # size of the grid box\n",
    "    dx = 2 * np.pi * constant['R'] * np.cos(2 * np.pi * latitude_ERAI /\n",
    "                                            360) / len(longitude_ERAI)\n",
    "    dy = np.pi * constant['R'] / 480\n",
    "    # calculate the sea ice area\n",
    "    SIC_ERAI_area = np.zeros(SIC_ERAI.shape, dtype=float)\n",
    "#     SFlux_ERAI_area = np.zeros(SFlux_ERAI.shape, dtype=float)\n",
    "    for i in np.arange(len(latitude_ERAI[:])):\n",
    "        # change the unit to terawatt\n",
    "        SIC_ERAI_area[:,:,i,:] = SIC_ERAI[:,:,i,:]* dx[i] * dy / 1E+6 # unit km2\n",
    "#         SFlux_ERAI_area[:,:,i,:] = SFlux_ERAI[:,:,i,:]* dx[i] * dy / 1E+12 # unit TeraWatt\n",
    "    SIC_ERAI_area[SIC_ERAI_area<0] = 0 # switch the mask from -1 to 0\n",
    "    print ('================  reshape input data into time series  =================')\n",
    "    SIC_ERAI_area_series = dlacs.preprocess.operator.unfold(SIC_ERAI_area)\n",
    "#     T2M_ERAI_series = dlacs.preprocess.operator.unfold(T2M_ERAI)\n",
    "#     SLP_ERAI_series = dlacs.preprocess.operator.unfold(SLP_ERAI)\n",
    "#     Z500_ERAI_series = dlacs.preprocess.operator.unfold(Z500_ERAI)\n",
    "#     Z850_ERAI_series = dlacs.preprocess.operator.unfold(Z850_ERAI)\n",
    "#     U10M_ERAI_series = dlacs.preprocess.operator.unfold(U10M_ERAI)\n",
    "#     V10M_ERAI_series = dlacs.preprocess.operator.unfold(V10M_ERAI)\n",
    "#     SFlux_ERAI_area_series = dlacs.preprocess.operator.unfold(SFlux_ERAI_area)\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select land-sea mask\n",
    "    sea_ice_mask_barents = sea_ice_mask_global[12:36,264:320]\n",
    "    print ('******************  choose the fields from target region  *******************')\n",
    "    # select the area between greenland and ice land for instance 60-70 N / 44-18 W\n",
    "    sic_exp = SIC_ERAI_area_series[:,12:36,264:320]\n",
    "#     t2m_exp = T2M_ERAI_series[:,12:36,264:320]\n",
    "#     slp_exp = SLP_ERAI_series[:,12:36,264:320]\n",
    "#     z500_exp = Z500_ERAI_series[:,12:36,264:320]\n",
    "#     z850_exp = Z850_ERAI_series[:,12:36,264:320]\n",
    "#     u10m_exp = U10M_ERAI_series[:,12:36,264:320]\n",
    "#     v10m_exp = V10M_ERAI_series[:,12:36,264:320]\n",
    "#     sflux_exp = SFlux_ERAI_area_series[:,12:36,264:320]\n",
    "    ohc_exp = OHC_300_ORAS4_weekly_series[:,12:36,264:320]\n",
    "    print(sic_exp.shape)\n",
    "#     print(t2m_exp.shape)\n",
    "#     print(slp_exp.shape)\n",
    "#     print(z500_exp.shape)\n",
    "#     print(u10m_exp.shape)\n",
    "#     print(v10m_exp.shape)\n",
    "#     print(sflux_exp.shape)\n",
    "    print(ohc_exp.shape)\n",
    "    print(latitude_ERAI[12:36])\n",
    "    print(longitude_ERAI[264:320])\n",
    "    print(latitude_ORAS4[12:36])\n",
    "    print(longitude_ORAS4[264:320])\n",
    "    #print(latitude_ERAI[26:40])\n",
    "    #print(longitude_ERAI[180:216])\n",
    "    #print(sic_exp[:])\n",
    "    print ('*******************  pre-processing  *********************')\n",
    "    print ('=========================   normalize data   ===========================')\n",
    "    sic_exp_norm = dlacs.preprocess.operator.normalize(sic_exp)\n",
    "#     t2m_exp_norm = deepclim.preprocess.operator.normalize(t2m_exp)\n",
    "#     slp_exp_norm = deepclim.preprocess.operator.normalize(slp_exp)\n",
    "#     z500_exp_norm = deepclim.preprocess.operator.normalize(z500_exp)\n",
    "#     z850_exp_norm = deepclim.preprocess.operator.normalize(z850_exp)\n",
    "#     u10m_exp_norm = deepclim.preprocess.operator.normalize(u10m_exp)\n",
    "#     v10m_exp_norm = deepclim.preprocess.operator.normalize(v10m_exp)\n",
    "#     sflux_exp_norm = deepclim.preprocess.operator.normalize(sflux_exp)\n",
    "    ohc_exp_norm = dlacs.preprocess.operator.normalize(ohc_exp)\n",
    "    print('================  save the normalizing factor  =================')\n",
    "    sic_max = np.amax(sic_exp)\n",
    "    sic_min = np.amin(sic_exp)\n",
    "    print(sic_max,\"km2\")\n",
    "    print(sic_min,\"km2\")\n",
    "    print ('====================    A series of time (index)    ====================')\n",
    "    _, yy, xx = sic_exp_norm.shape # get the lat lon dimension\n",
    "    year = np.arange(1979,2017,1)\n",
    "    year_cycle = np.repeat(year,48)\n",
    "    month_cycle = np.repeat(np.arange(1,13,1),4)\n",
    "    month_cycle = np.tile(month_cycle,len(year)+1) # one extra repeat for lead time dependent prediction\n",
    "    month_cycle.astype(float)\n",
    "    month_2D = np.repeat(month_cycle[:,np.newaxis],yy,1)\n",
    "    month_exp = np.repeat(month_2D[:,:,np.newaxis],xx,2)\n",
    "    print ('===================  artificial data for evaluation ====================')\n",
    "    # calculate climatology of SIC\n",
    "#     seansonal_cycle_SIC = np.zeros(48,dtype=float)\n",
    "#     for i in np.arange(48):\n",
    "#         seansonal_cycle_SIC[i] = np.mean(SIC_ERAI_sum_norm[i::48],axis=0)\n",
    "    # weight for loss\n",
    "#     weight_month = np.array([0,1,1,\n",
    "#                              1,0,0,\n",
    "#                              1,1,1,\n",
    "#                              0,0,0])\n",
    "    #weight_loss = np.repeat(weight_month,4)\n",
    "    #weight_loss = np.tile(weight_loss,len(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure for LSTM <br>\n",
    "** We use Pytorth to implement LSTM neural network with time series of climate data. ** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************  parameter for check  *********************\n",
      "*******************  create basic dimensions for tensor and network  *********************\n",
      "*******************  cross validation and testing data  *********************\n",
      "*******************  check the environment  *********************\n",
      "Pytorch version 1.1.0\n",
      "Is CUDA available? False\n",
      "*******************  run BayesConvLSTM  *********************\n",
      "The model is designed to make many to one prediction.\n",
      "A series of multi-chanel variables will be input to the model.\n",
      "The model learns by verifying the output at each timestep.\n",
      "BayesConvLSTM(\n",
      "  (cell0): BayesConvLSTMCell()\n",
      "  (cell1): BayesConvLSTMCell()\n",
      "  (cell2): BayesConvLSTMCell()\n",
      ")\n",
      "ELBO(\n",
      "  (loss_function): MSELoss()\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "    print ('*******************  parameter for check  *********************')\n",
    "    choice_exp_norm = ohc_exp_norm\n",
    "    print ('*******************  create basic dimensions for tensor and network  *********************')\n",
    "    # specifications of neural network\n",
    "    input_channels = 3\n",
    "    hidden_channels = [3, 2, 1] # number of channels & hidden layers, the channels of last layer is the channels of output, too\n",
    "    #hidden_channels = [3, 3, 3, 3, 2]\n",
    "    #hidden_channels = [2]\n",
    "    kernel_size = 3\n",
    "    # here we input a sequence and predict the next step only\n",
    "    #step = 1 # how many steps to predict ahead\n",
    "    #effective_step = [0] # step to output\n",
    "    batch_size = 1\n",
    "    #num_layers = 1\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 150#0\n",
    "    print ('*******************  cross validation and testing data  *********************')\n",
    "    # take 10% data as cross-validation data\n",
    "    cross_valid_year = 4\n",
    "    # take 10% years as testing data\n",
    "    test_year = 4\n",
    "    # minibatch\n",
    "    #iterations = 3 # training data divided into 3 sets\n",
    "    print ('*******************  check the environment  *********************')\n",
    "    print (\"Pytorch version {}\".format(torch.__version__))\n",
    "    # check if CUDA is available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"Is CUDA available? {}\".format(use_cuda))\n",
    "    # CUDA settings torch.__version__ must > 0.4\n",
    "    # !!! This is important for the model!!! The first option is gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   \n",
    "    print ('*******************  run BayesConvLSTM  *********************')\n",
    "    print ('The model is designed to make many to one prediction.')\n",
    "    print ('A series of multi-chanel variables will be input to the model.')\n",
    "    print ('The model learns by verifying the output at each timestep.')\n",
    "    # check the sequence length\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    # initialize our model\n",
    "    model = dlacs.BayesConvLSTM.BayesConvLSTM(input_channels, hidden_channels, kernel_size).to(device)\n",
    "    # use Evidence Lower Bound (ELBO) to quantify the loss\n",
    "    ELBO = dlacs.function.ELBO(height*width)\n",
    "    # for classification, target must be integers (label)\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=torch.nn.KLDivLoss())\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=torch.CrossEntropyLoss())\n",
    "    #ELBO = dlacs.function.ELBO(height*width,loss_function=torch.NLLLoss(reduction='mean'))\n",
    "    # penalty for kl\n",
    "    penalty_kl = sequence_len\n",
    "    # stochastic gradient descent\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # Adam optimizer\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(model)\n",
    "    print(ELBO)\n",
    "    print(optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell0.Wxi_mu\n",
      "tensor([[[[-1.0558e-01, -1.8014e-01,  3.3321e-02],\n",
      "          [ 2.3716e-02, -1.2316e-01,  1.5326e-01],\n",
      "          [-1.4931e-01, -1.2037e-01, -2.9395e-03]],\n",
      "\n",
      "         [[-1.6941e-01,  1.2226e-01, -6.5187e-02],\n",
      "          [-1.0834e-01, -1.4857e-01, -1.3921e-04],\n",
      "          [-1.3112e-01,  1.2654e-01, -6.7747e-02]],\n",
      "\n",
      "         [[ 1.7094e-01, -4.0709e-02,  1.6852e-01],\n",
      "          [ 6.5585e-03, -1.6063e-01,  1.8219e-01],\n",
      "          [ 1.2504e-01,  9.6949e-02, -7.3622e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3706e-01, -1.2122e-01,  6.7758e-02],\n",
      "          [ 8.9354e-02,  1.5358e-01,  1.2304e-01],\n",
      "          [-6.3727e-02,  7.3968e-02, -1.8771e-01]],\n",
      "\n",
      "         [[ 1.2527e-01, -1.1710e-02, -7.6872e-02],\n",
      "          [ 3.5652e-03,  1.4838e-01, -5.1766e-02],\n",
      "          [ 1.2488e-02,  1.5517e-02, -1.2831e-02]],\n",
      "\n",
      "         [[ 1.7406e-01,  1.8051e-01,  4.7447e-02],\n",
      "          [ 9.8560e-02, -1.8229e-03, -1.1217e-01],\n",
      "          [ 5.5913e-03,  4.4141e-03, -1.0908e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8881e-02, -5.0010e-02,  1.7787e-01],\n",
      "          [ 7.9141e-02,  6.8945e-02, -1.5756e-01],\n",
      "          [ 1.5758e-01, -1.7167e-01, -9.1566e-02]],\n",
      "\n",
      "         [[ 9.8564e-02,  1.4169e-01, -4.1926e-02],\n",
      "          [-1.2288e-03, -1.0169e-01,  1.8602e-01],\n",
      "          [ 1.7055e-01, -6.6630e-02, -1.7616e-02]],\n",
      "\n",
      "         [[ 1.0433e-01, -6.5166e-02, -7.2140e-02],\n",
      "          [ 1.2388e-01, -1.6532e-01, -1.3893e-01],\n",
      "          [-7.9466e-02,  2.5198e-02, -1.2264e-01]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Whi_mu\n",
      "tensor([[[[ 0.1505,  0.1594,  0.1507],\n",
      "          [-0.1064, -0.1529,  0.0767],\n",
      "          [-0.0612, -0.0013, -0.0842]],\n",
      "\n",
      "         [[-0.1894, -0.1463,  0.0028],\n",
      "          [ 0.0290,  0.0451, -0.0291],\n",
      "          [ 0.1440, -0.0578, -0.1537]],\n",
      "\n",
      "         [[-0.1109, -0.0751,  0.0480],\n",
      "          [ 0.1751, -0.0272, -0.1116],\n",
      "          [ 0.0330, -0.1047,  0.0708]]],\n",
      "\n",
      "\n",
      "        [[[-0.0114,  0.1160,  0.1861],\n",
      "          [-0.0486, -0.0762,  0.0408],\n",
      "          [-0.1535,  0.1055, -0.1539]],\n",
      "\n",
      "         [[ 0.1408,  0.0064,  0.1595],\n",
      "          [ 0.1489, -0.0719, -0.1464],\n",
      "          [ 0.1878,  0.0596, -0.1849]],\n",
      "\n",
      "         [[-0.0016,  0.1308, -0.0777],\n",
      "          [ 0.0976,  0.1232,  0.0970],\n",
      "          [-0.0749,  0.0472,  0.1033]]],\n",
      "\n",
      "\n",
      "        [[[-0.0918, -0.0488, -0.1228],\n",
      "          [ 0.1324, -0.0902,  0.0612],\n",
      "          [-0.0488,  0.1842, -0.1201]],\n",
      "\n",
      "         [[ 0.0284, -0.0162, -0.0321],\n",
      "          [ 0.1476, -0.1896,  0.0177],\n",
      "          [ 0.0410, -0.1147, -0.1073]],\n",
      "\n",
      "         [[ 0.0245,  0.0552,  0.0043],\n",
      "          [-0.0861,  0.0531, -0.0379],\n",
      "          [-0.0255, -0.0181,  0.1451]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxf_mu\n",
      "tensor([[[[ 0.1483, -0.1208,  0.0284],\n",
      "          [ 0.0200, -0.0550, -0.1284],\n",
      "          [ 0.1364,  0.1188, -0.0278]],\n",
      "\n",
      "         [[ 0.0073,  0.0267,  0.1735],\n",
      "          [-0.0877,  0.1521,  0.1651],\n",
      "          [ 0.1610,  0.1607,  0.0480]],\n",
      "\n",
      "         [[-0.0760,  0.0976,  0.1390],\n",
      "          [ 0.1454,  0.1334, -0.0760],\n",
      "          [ 0.1114,  0.1230, -0.0989]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0878, -0.1020, -0.0035],\n",
      "          [ 0.1706, -0.0822,  0.0816],\n",
      "          [-0.0940, -0.0863,  0.0037]],\n",
      "\n",
      "         [[ 0.0222, -0.1566, -0.1362],\n",
      "          [-0.0844, -0.1022,  0.0365],\n",
      "          [-0.1225,  0.1563,  0.0662]],\n",
      "\n",
      "         [[ 0.0304, -0.1415, -0.0393],\n",
      "          [-0.1836, -0.0835, -0.0122],\n",
      "          [-0.0296, -0.0322,  0.1411]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0464, -0.1525,  0.1017],\n",
      "          [-0.0647,  0.0233, -0.1618],\n",
      "          [ 0.1650,  0.0671,  0.1499]],\n",
      "\n",
      "         [[-0.0495, -0.1173, -0.1195],\n",
      "          [-0.1611,  0.0425, -0.1830],\n",
      "          [ 0.1597,  0.0825, -0.0109]],\n",
      "\n",
      "         [[ 0.0979,  0.0147,  0.0652],\n",
      "          [ 0.1144,  0.0251,  0.0099],\n",
      "          [-0.0434,  0.0506, -0.1574]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Whf_mu\n",
      "tensor([[[[-0.0477,  0.1021,  0.1133],\n",
      "          [ 0.1019, -0.1736, -0.1368],\n",
      "          [ 0.1167, -0.1414,  0.0797]],\n",
      "\n",
      "         [[ 0.0577,  0.1485, -0.0892],\n",
      "          [-0.0392, -0.0002, -0.0093],\n",
      "          [ 0.0752,  0.0591, -0.1756]],\n",
      "\n",
      "         [[-0.1076,  0.0940, -0.0927],\n",
      "          [-0.0192, -0.0187, -0.0515],\n",
      "          [-0.0295,  0.1501,  0.0820]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0443,  0.0992, -0.0708],\n",
      "          [ 0.1801,  0.0410, -0.1901],\n",
      "          [ 0.1426,  0.1340,  0.1905]],\n",
      "\n",
      "         [[-0.1884,  0.0514,  0.1214],\n",
      "          [ 0.1535, -0.1456,  0.0904],\n",
      "          [ 0.1441, -0.1416,  0.0159]],\n",
      "\n",
      "         [[ 0.0998, -0.0896,  0.0262],\n",
      "          [ 0.1321,  0.1180,  0.1093],\n",
      "          [ 0.1409,  0.0826, -0.1799]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0978,  0.1783,  0.0761],\n",
      "          [-0.0497,  0.1332,  0.1554],\n",
      "          [-0.0412,  0.0712,  0.1806]],\n",
      "\n",
      "         [[-0.1848,  0.0492,  0.1601],\n",
      "          [ 0.1624,  0.0586, -0.1298],\n",
      "          [ 0.0999,  0.0259,  0.0918]],\n",
      "\n",
      "         [[ 0.1636, -0.0670, -0.0600],\n",
      "          [-0.0835, -0.1121, -0.0713],\n",
      "          [-0.1210,  0.0723, -0.0130]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxc_mu\n",
      "tensor([[[[-0.0057, -0.0202, -0.0485],\n",
      "          [-0.0967,  0.1473, -0.0356],\n",
      "          [-0.0162, -0.0720, -0.1881]],\n",
      "\n",
      "         [[-0.0987, -0.0084,  0.1601],\n",
      "          [ 0.1424, -0.1698, -0.0228],\n",
      "          [-0.1683,  0.1419, -0.0611]],\n",
      "\n",
      "         [[-0.1773, -0.1575,  0.1331],\n",
      "          [ 0.1831,  0.1064,  0.1431],\n",
      "          [-0.0737, -0.0624, -0.0597]]],\n",
      "\n",
      "\n",
      "        [[[-0.1334,  0.0719,  0.1027],\n",
      "          [ 0.1069, -0.1807, -0.1687],\n",
      "          [-0.1189, -0.1354, -0.1711]],\n",
      "\n",
      "         [[ 0.1653, -0.0884,  0.0597],\n",
      "          [-0.1120, -0.1139,  0.1678],\n",
      "          [ 0.1285, -0.0140,  0.0315]],\n",
      "\n",
      "         [[ 0.0852, -0.0090,  0.0941],\n",
      "          [-0.1844, -0.1168,  0.1344],\n",
      "          [-0.0234,  0.0558, -0.0977]]],\n",
      "\n",
      "\n",
      "        [[[-0.0627, -0.1281,  0.1190],\n",
      "          [-0.1080, -0.1252,  0.1044],\n",
      "          [-0.0424,  0.0172,  0.0418]],\n",
      "\n",
      "         [[-0.0770,  0.0221, -0.0586],\n",
      "          [-0.0362, -0.1121, -0.0629],\n",
      "          [ 0.1090, -0.0433, -0.1642]],\n",
      "\n",
      "         [[-0.0198, -0.1160,  0.0252],\n",
      "          [-0.0019,  0.1053,  0.0765],\n",
      "          [-0.0321, -0.1601,  0.0124]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Whc_mu\n",
      "tensor([[[[ 0.0359,  0.1472,  0.1581],\n",
      "          [-0.1290, -0.0004,  0.0275],\n",
      "          [-0.0004,  0.0909, -0.1282]],\n",
      "\n",
      "         [[ 0.0801,  0.0296, -0.1655],\n",
      "          [ 0.1732,  0.0651, -0.0822],\n",
      "          [-0.0551, -0.1902,  0.1069]],\n",
      "\n",
      "         [[-0.1731,  0.1697, -0.1525],\n",
      "          [-0.0669, -0.0647, -0.1256],\n",
      "          [ 0.0472, -0.1045, -0.0320]]],\n",
      "\n",
      "\n",
      "        [[[-0.1885, -0.1612,  0.1851],\n",
      "          [ 0.1781, -0.1923,  0.0423],\n",
      "          [ 0.1899,  0.1653, -0.0224]],\n",
      "\n",
      "         [[-0.0145,  0.0156, -0.0249],\n",
      "          [-0.1112, -0.0055, -0.0352],\n",
      "          [ 0.0536,  0.0455,  0.1236]],\n",
      "\n",
      "         [[-0.0340, -0.1064, -0.0361],\n",
      "          [-0.0933,  0.0180,  0.0059],\n",
      "          [ 0.0303, -0.1235,  0.0433]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0081,  0.1448,  0.1891],\n",
      "          [ 0.1265,  0.1450, -0.1162],\n",
      "          [-0.0990, -0.1284,  0.1863]],\n",
      "\n",
      "         [[-0.0763, -0.1483, -0.0290],\n",
      "          [ 0.1244,  0.0433, -0.1442],\n",
      "          [-0.0900, -0.1422,  0.1674]],\n",
      "\n",
      "         [[-0.0737,  0.1878, -0.1239],\n",
      "          [-0.0063, -0.0742, -0.1087],\n",
      "          [ 0.0585,  0.1563, -0.0684]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxo_mu\n",
      "tensor([[[[-0.0006, -0.1884, -0.0080],\n",
      "          [-0.1371,  0.0886,  0.1734],\n",
      "          [ 0.0219, -0.1607,  0.1174]],\n",
      "\n",
      "         [[-0.1835,  0.1636,  0.0334],\n",
      "          [ 0.0926,  0.1284,  0.1311],\n",
      "          [ 0.1581,  0.1168, -0.1597]],\n",
      "\n",
      "         [[ 0.1251,  0.0225, -0.1540],\n",
      "          [-0.0835, -0.1881, -0.1867],\n",
      "          [-0.1786, -0.1777, -0.0792]]],\n",
      "\n",
      "\n",
      "        [[[-0.0390,  0.1583, -0.1335],\n",
      "          [-0.0334, -0.0339, -0.1427],\n",
      "          [-0.0163,  0.0857, -0.1614]],\n",
      "\n",
      "         [[ 0.0259,  0.0045,  0.0927],\n",
      "          [-0.1736, -0.0424,  0.1609],\n",
      "          [ 0.1830,  0.0239, -0.0536]],\n",
      "\n",
      "         [[ 0.1002, -0.0555, -0.1863],\n",
      "          [ 0.0137,  0.0472,  0.1629],\n",
      "          [ 0.1072, -0.0688, -0.1815]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1561,  0.1189, -0.1054],\n",
      "          [-0.0654,  0.0633,  0.0789],\n",
      "          [-0.0077, -0.0585,  0.1569]],\n",
      "\n",
      "         [[ 0.0418,  0.0187, -0.1913],\n",
      "          [ 0.1730,  0.0185,  0.0857],\n",
      "          [-0.1578, -0.0213,  0.1000]],\n",
      "\n",
      "         [[-0.1295, -0.0706, -0.0222],\n",
      "          [ 0.0388,  0.0221, -0.0255],\n",
      "          [ 0.1079,  0.0573, -0.0127]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Who_mu\n",
      "tensor([[[[-0.1418,  0.1296,  0.1494],\n",
      "          [-0.1297,  0.0966, -0.1601],\n",
      "          [ 0.0589, -0.1354,  0.1033]],\n",
      "\n",
      "         [[-0.1342,  0.0837, -0.0151],\n",
      "          [-0.0918, -0.0557,  0.1827],\n",
      "          [ 0.1472,  0.1182,  0.0730]],\n",
      "\n",
      "         [[-0.0610,  0.1449, -0.1682],\n",
      "          [ 0.0557,  0.1713, -0.1626],\n",
      "          [-0.1836, -0.1490,  0.0024]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1771, -0.0781, -0.1357],\n",
      "          [-0.0530,  0.1234,  0.0002],\n",
      "          [ 0.0061,  0.0391,  0.1904]],\n",
      "\n",
      "         [[ 0.0308,  0.0320,  0.0450],\n",
      "          [ 0.1424, -0.1130, -0.1656],\n",
      "          [ 0.0455,  0.1137,  0.1095]],\n",
      "\n",
      "         [[-0.1099,  0.0360, -0.0398],\n",
      "          [-0.0466,  0.1522,  0.0596],\n",
      "          [-0.1342,  0.1874,  0.1896]]],\n",
      "\n",
      "\n",
      "        [[[-0.1889, -0.0500, -0.0976],\n",
      "          [-0.0876, -0.0597, -0.1398],\n",
      "          [-0.0772, -0.1107, -0.0526]],\n",
      "\n",
      "         [[ 0.0040, -0.0116, -0.1339],\n",
      "          [ 0.1082,  0.0162,  0.0092],\n",
      "          [-0.0347, -0.0388, -0.0249]],\n",
      "\n",
      "         [[-0.0752,  0.1624, -0.0845],\n",
      "          [ 0.1130,  0.0603, -0.0400],\n",
      "          [-0.0488,  0.1633,  0.1463]]]])\n",
      "torch.Size([3, 3, 3, 3])\n",
      "=========================\n",
      "cell0.Wxi_bias\n",
      "tensor([0.0449, 0.1731, 0.0882])\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxf_bias\n",
      "tensor([ 0.1077,  0.0077, -0.0217])\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxc_bias\n",
      "tensor([ 0.0084, -0.1140,  0.0357])\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxo_bias\n",
      "tensor([ 0.1414, -0.0581, -0.1559])\n",
      "torch.Size([3])\n",
      "=========================\n",
      "cell0.Wxi_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Whi_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Wxf_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Whf_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Wxc_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Whc_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Wxo_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell0.Who_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxi_mu\n",
      "tensor([[[[-0.1404,  0.0992,  0.0712],\n",
      "          [ 0.1500, -0.0049, -0.1585],\n",
      "          [ 0.0450,  0.0732,  0.1892]],\n",
      "\n",
      "         [[ 0.0362, -0.1619,  0.1274],\n",
      "          [ 0.1129, -0.0810,  0.1015],\n",
      "          [-0.0978,  0.0430, -0.0125]],\n",
      "\n",
      "         [[-0.1353, -0.0158,  0.1046],\n",
      "          [-0.1306, -0.0575, -0.0956],\n",
      "          [ 0.1300, -0.1472, -0.1036]]],\n",
      "\n",
      "\n",
      "        [[[-0.0360, -0.1872,  0.0382],\n",
      "          [-0.1581, -0.1454, -0.0130],\n",
      "          [ 0.0750, -0.0926,  0.0594]],\n",
      "\n",
      "         [[ 0.0781, -0.1598, -0.1888],\n",
      "          [ 0.1771, -0.0096, -0.0051],\n",
      "          [-0.0063,  0.1794,  0.1344]],\n",
      "\n",
      "         [[-0.1107,  0.1619, -0.1914],\n",
      "          [-0.1832,  0.1638, -0.0908],\n",
      "          [ 0.0348,  0.1907,  0.0779]]]])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Whi_mu\n",
      "tensor([[[[-0.0060,  0.1114,  0.1306],\n",
      "          [ 0.1781,  0.1436,  0.1708],\n",
      "          [-0.1407,  0.1659,  0.0237]],\n",
      "\n",
      "         [[-0.0003,  0.1765, -0.1130],\n",
      "          [ 0.0117,  0.1632,  0.0438],\n",
      "          [-0.1570, -0.1841,  0.0727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0648, -0.0630,  0.0190],\n",
      "          [-0.0354, -0.1580, -0.0695],\n",
      "          [-0.0177,  0.1496,  0.0449]],\n",
      "\n",
      "         [[ 0.1066,  0.1308,  0.1854],\n",
      "          [ 0.0280, -0.0853,  0.0243],\n",
      "          [-0.1214, -0.1760, -0.1087]]]])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxf_mu\n",
      "tensor([[[[ 1.8089e-01,  4.6241e-02, -1.5846e-02],\n",
      "          [-5.0193e-02, -1.3801e-01, -2.7986e-03],\n",
      "          [-1.3488e-01, -1.6418e-01, -4.1823e-02]],\n",
      "\n",
      "         [[-1.7122e-01, -1.0898e-01,  7.3998e-02],\n",
      "          [ 1.0235e-01,  1.4572e-02,  1.7708e-01],\n",
      "          [ 1.8915e-01, -1.5504e-01,  1.1076e-01]],\n",
      "\n",
      "         [[ 1.3813e-01, -1.3368e-02,  3.3334e-02],\n",
      "          [-3.3073e-02,  7.7808e-02, -1.1030e-01],\n",
      "          [-3.9036e-02, -5.6320e-02,  2.0027e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5672e-01, -9.8987e-02,  4.5164e-02],\n",
      "          [ 1.1657e-01,  6.4120e-05, -1.3413e-01],\n",
      "          [-3.0016e-02,  8.5811e-04, -1.7133e-01]],\n",
      "\n",
      "         [[ 4.8487e-02, -1.0534e-01, -1.4458e-01],\n",
      "          [ 9.2448e-02, -9.0670e-03,  6.7324e-02],\n",
      "          [-1.4408e-01,  1.4100e-01, -3.7544e-02]],\n",
      "\n",
      "         [[-1.0125e-01,  8.9484e-02, -1.2644e-01],\n",
      "          [ 6.6773e-02,  1.3655e-01, -6.9855e-03],\n",
      "          [-7.1238e-02, -2.5733e-03,  1.5021e-01]]]])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Whf_mu\n",
      "tensor([[[[-0.0731, -0.0869, -0.0946],\n",
      "          [-0.0206,  0.1421, -0.1010],\n",
      "          [-0.1215, -0.1730, -0.0944]],\n",
      "\n",
      "         [[-0.0532,  0.1439,  0.0485],\n",
      "          [-0.0335,  0.1732,  0.1497],\n",
      "          [-0.0009, -0.1924,  0.0016]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0568, -0.0796,  0.0265],\n",
      "          [-0.0356,  0.0204, -0.1383],\n",
      "          [-0.1770, -0.0752, -0.1090]],\n",
      "\n",
      "         [[ 0.1912, -0.0911,  0.0309],\n",
      "          [-0.1404,  0.1690, -0.0901],\n",
      "          [-0.1060, -0.0982, -0.0329]]]])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxc_mu\n",
      "tensor([[[[-0.0222,  0.1284,  0.1002],\n",
      "          [ 0.1631, -0.1533,  0.0350],\n",
      "          [ 0.1828, -0.1165,  0.0352]],\n",
      "\n",
      "         [[-0.0629,  0.1134, -0.0560],\n",
      "          [-0.0066,  0.0927,  0.1167],\n",
      "          [-0.1769,  0.0404, -0.1750]],\n",
      "\n",
      "         [[-0.1779,  0.0923, -0.0512],\n",
      "          [-0.1366,  0.0809,  0.1696],\n",
      "          [-0.1831, -0.0611,  0.0865]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0611, -0.0180, -0.1381],\n",
      "          [-0.1713, -0.0810, -0.0440],\n",
      "          [-0.1204,  0.1552,  0.1176]],\n",
      "\n",
      "         [[-0.1674,  0.0036,  0.1364],\n",
      "          [ 0.1514,  0.1678,  0.1255],\n",
      "          [-0.1083, -0.1702,  0.1447]],\n",
      "\n",
      "         [[ 0.1181,  0.0820, -0.1202],\n",
      "          [ 0.0926, -0.1522,  0.1392],\n",
      "          [-0.0532, -0.0579,  0.0461]]]])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Whc_mu\n",
      "tensor([[[[-0.1240,  0.1753,  0.0938],\n",
      "          [-0.1022,  0.1047, -0.1400],\n",
      "          [-0.1333, -0.1533,  0.0638]],\n",
      "\n",
      "         [[ 0.1651, -0.0441,  0.0830],\n",
      "          [-0.1517, -0.0811,  0.0503],\n",
      "          [-0.1181, -0.0214,  0.1019]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0463, -0.1071,  0.0453],\n",
      "          [-0.0066, -0.1868, -0.1811],\n",
      "          [-0.1088,  0.0688,  0.0407]],\n",
      "\n",
      "         [[-0.0922, -0.1232, -0.1846],\n",
      "          [ 0.0690, -0.1158,  0.1553],\n",
      "          [-0.1189,  0.0874, -0.0758]]]])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxo_mu\n",
      "tensor([[[[ 0.1527, -0.1640,  0.0508],\n",
      "          [ 0.0962, -0.0682, -0.0372],\n",
      "          [-0.1589,  0.1528,  0.0121]],\n",
      "\n",
      "         [[ 0.1067,  0.0870, -0.0544],\n",
      "          [-0.0982,  0.0931,  0.0710],\n",
      "          [-0.0285, -0.1295, -0.0802]],\n",
      "\n",
      "         [[-0.0122,  0.0565, -0.1265],\n",
      "          [ 0.0528, -0.0288,  0.0336],\n",
      "          [ 0.0388,  0.1410, -0.0516]]],\n",
      "\n",
      "\n",
      "        [[[-0.0034,  0.1200, -0.0667],\n",
      "          [-0.1649, -0.1221, -0.0091],\n",
      "          [ 0.0953,  0.0078, -0.0263]],\n",
      "\n",
      "         [[-0.0107, -0.0407,  0.0003],\n",
      "          [ 0.0730,  0.0168,  0.0869],\n",
      "          [ 0.0928, -0.1398, -0.1093]],\n",
      "\n",
      "         [[ 0.1082,  0.1917,  0.0315],\n",
      "          [ 0.1872,  0.1334,  0.0763],\n",
      "          [ 0.1679,  0.0947,  0.0454]]]])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "=========================\n",
      "cell1.Who_mu\n",
      "tensor([[[[-0.0509,  0.0025, -0.1129],\n",
      "          [ 0.0715,  0.0297,  0.0817],\n",
      "          [-0.0365,  0.0852, -0.1337]],\n",
      "\n",
      "         [[ 0.1213, -0.0770,  0.0929],\n",
      "          [-0.0727, -0.1370, -0.0354],\n",
      "          [-0.1425,  0.1710, -0.1473]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1704,  0.1538,  0.1436],\n",
      "          [-0.0588,  0.1103, -0.1793],\n",
      "          [-0.0882, -0.0427, -0.1215]],\n",
      "\n",
      "         [[ 0.0544, -0.1143,  0.0111],\n",
      "          [-0.1670,  0.0416,  0.0137],\n",
      "          [-0.0586, -0.0527, -0.0813]]]])\n",
      "torch.Size([2, 2, 3, 3])\n",
      "=========================\n",
      "cell1.Wxi_bias\n",
      "tensor([0.1286, 0.0473])\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxf_bias\n",
      "tensor([-0.0822, -0.1662])\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxc_bias\n",
      "tensor([ 0.1198, -0.1193])\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxo_bias\n",
      "tensor([-0.0934, -0.1523])\n",
      "torch.Size([2])\n",
      "=========================\n",
      "cell1.Wxi_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Whi_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxf_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Whf_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxc_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Whc_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Wxo_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell1.Who_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxi_mu\n",
      "tensor([[[[-0.1606,  0.0787,  0.1289],\n",
      "          [-0.0449,  0.0370, -0.2151],\n",
      "          [ 0.2200,  0.0090, -0.2147]],\n",
      "\n",
      "         [[ 0.0652, -0.0208,  0.1788],\n",
      "          [-0.1789, -0.0495,  0.1428],\n",
      "          [ 0.2312,  0.2153, -0.1653]]]])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Whi_mu\n",
      "tensor([[[[ 0.0746, -0.1864, -0.0186],\n",
      "          [-0.1168,  0.0553, -0.0861],\n",
      "          [-0.1042, -0.1182,  0.1272]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxf_mu\n",
      "tensor([[[[-0.2300,  0.2292,  0.2332],\n",
      "          [ 0.0055, -0.1534,  0.1039],\n",
      "          [ 0.0683, -0.1853,  0.1626]],\n",
      "\n",
      "         [[ 0.0172,  0.1180,  0.0007],\n",
      "          [-0.0547, -0.2348,  0.2096],\n",
      "          [-0.0505,  0.0312,  0.1056]]]])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Whf_mu\n",
      "tensor([[[[-0.1685,  0.0699, -0.1502],\n",
      "          [-0.0669, -0.0573, -0.1454],\n",
      "          [ 0.1328,  0.0575, -0.0792]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxc_mu\n",
      "tensor([[[[ 0.0426,  0.1938,  0.1131],\n",
      "          [ 0.1673, -0.1285,  0.1556],\n",
      "          [ 0.0333, -0.0428, -0.0106]],\n",
      "\n",
      "         [[ 0.1117,  0.0892, -0.0455],\n",
      "          [-0.0538, -0.1840, -0.2090],\n",
      "          [ 0.2158,  0.1657, -0.1914]]]])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Whc_mu\n",
      "tensor([[[[-0.1140,  0.1616, -0.0362],\n",
      "          [ 0.2270, -0.2153, -0.0376],\n",
      "          [-0.0136, -0.0810, -0.2291]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxo_mu\n",
      "tensor([[[[-0.0806,  0.1536,  0.0378],\n",
      "          [-0.1392, -0.0544, -0.0027],\n",
      "          [ 0.0450,  0.1036,  0.0378]],\n",
      "\n",
      "         [[-0.1259, -0.1299, -0.0302],\n",
      "          [ 0.0685,  0.1393, -0.0400],\n",
      "          [-0.1050, -0.1544,  0.2353]]]])\n",
      "torch.Size([1, 2, 3, 3])\n",
      "=========================\n",
      "cell2.Who_mu\n",
      "tensor([[[[ 0.0191,  0.0763,  0.0068],\n",
      "          [ 0.1463,  0.2203,  0.1838],\n",
      "          [-0.1304,  0.0951,  0.0110]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "=========================\n",
      "cell2.Wxi_bias\n",
      "tensor([0.1835])\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxf_bias\n",
      "tensor([-0.2281])\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxc_bias\n",
      "tensor([-0.0220])\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxo_bias\n",
      "tensor([-0.0880])\n",
      "torch.Size([1])\n",
      "=========================\n",
      "cell2.Wxi_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Whi_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxf_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Whf_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxc_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Whc_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Wxo_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n",
      "cell2.Who_log_alpha\n",
      "tensor([[-3.]])\n",
      "torch.Size([1, 1])\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print (name)\n",
    "            print (param.data)\n",
    "            print (param.size())\n",
    "            print (\"=========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('##############################################################')\n",
    "    print('#############  preview model parameters matrix  ###############')\n",
    "    print('##############################################################')\n",
    "    print('Number of parameter matrices: ', len(list(model.parameters())))\n",
    "    for i in range(len(list(model.parameters()))):\n",
    "        print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "##################  start training loop  #####################\n",
      "##############################################################\n",
      "Epoch  0 MSE:  89783.515625\n",
      "Epoch  5 MSE:  59760.1484375\n",
      "Epoch  10 MSE:  39905.62890625\n",
      "Epoch  15 MSE:  33778.2265625\n",
      "Epoch  20 MSE:  29385.525390625\n",
      "Epoch  25 MSE:  26195.1796875\n",
      "Epoch  30 MSE:  19809.05859375\n",
      "Epoch  35 MSE:  18534.54296875\n",
      "Epoch  40 MSE:  16601.708984375\n",
      "Epoch  45 MSE:  15840.9091796875\n",
      "Epoch  50 MSE:  15043.3095703125\n",
      "Epoch  55 MSE:  14217.677734375\n",
      "Epoch  60 MSE:  13625.8056640625\n",
      "Epoch  65 MSE:  13082.3955078125\n",
      "Epoch  70 MSE:  12588.7265625\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    print('##############################################################')\n",
    "    print('##################  start training loop  #####################')\n",
    "    print('##############################################################')\n",
    "    # track training loss\n",
    "    hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                choice_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).to(device)\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).to(device)\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, kl_loss, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_target = y_var\n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            #print (timestep)\n",
    "            if timestep == 0:\n",
    "                loss = ELBO(y_pred, y_target, kl_loss,\n",
    "                            1 / (len(hidden_channels) * 8 * penalty_kl)) # weight of KL due to 8 gates in each layer\n",
    "            else:\n",
    "                loss += ELBO(y_pred, y_target, kl_loss,\n",
    "                             1 / (len(hidden_channels) * 8 * penalty_kl))            \n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parametersdd\n",
    "        optimiser.step()\n",
    "        \n",
    "    # save the model\n",
    "    # (recommended) save the model parameters only\n",
    "    torch.save(model.state_dict(), os.path.join(output_path,'bayesconvlstm.pkl'))\n",
    "    # save the entire model\n",
    "    # torch.save(model, os.path.join(output_path,'bayesconvlstm.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (\"*******************  Loss with time  **********************\")\n",
    "    fig00 = plt.figure()\n",
    "    try:\n",
    "        plt.plot(hist, label=\"Training loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        fig00.savefig(os.path.join(output_path,'SIC_ERAI_LSTM_pred_error.png'),dpi=200)\n",
    "    except:\n",
    "        print('Model is reloaded instead of trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  evaluation matrix  *********************')\n",
    "    # The prediction will be evaluated through RMSE against climatology\n",
    "    \n",
    "    # error score for temporal-spatial fields, without keeping spatial pattern\n",
    "    def RMSE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the RMSE. x is input series and y is reference series.\n",
    "        It calculates RMSE over the domain, not over time. The spatial structure\n",
    "        will not be kept.\n",
    "        Parameter\n",
    "        ----------------------\n",
    "        x: input time series with the shape [time, lat, lon]\n",
    "        \"\"\"\n",
    "        x_series = x.reshape(x.shape[0],-1)\n",
    "        y_series = y.reshape(y.shape[0],-1)\n",
    "        rmse = np.sqrt(np.mean((x_series - y_series)**2,1))\n",
    "        rmse_std = np.sqrt(np.std((x_series - y_series)**2,1))\n",
    "    \n",
    "        return rmse, rmse_std\n",
    "    \n",
    "    # error score for temporal-spatial fields, keeping spatial pattern\n",
    "    def MAE(x,y):\n",
    "        \"\"\"\n",
    "        Calculate the MAE. x is input series and y is reference series.\n",
    "        It calculate MAE over time and keeps the spatial structure.\n",
    "        \"\"\"\n",
    "        mae = np.mean(np.abs(x-y),0)\n",
    "        \n",
    "        return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    %%time\n",
    "    #################################################################################\n",
    "    ########  operational lead time dependent prediction with testing data   ########\n",
    "    #################################################################################\n",
    "    print('##############################################################')\n",
    "    print('###################  start prediction loop ###################')\n",
    "    print('##############################################################')\n",
    "    # the model learn from time series and try to predict the next time step based on the previous time series\n",
    "    print ('*******************************  one step ahead forecast  *********************************')\n",
    "    print ('************  the last {} years of total time series are treated as test data  ************'.format(test_year))\n",
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    choice_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        choice_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1, training=False)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week2month(series, m):\n",
    "    \"\"\"\n",
    "    Select certain month from yearly data at weekly resolution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array-like\n",
    "        Three-dimensional numeric arrays with time as the first dimenison [time, lat, lon]\n",
    "    m: int\n",
    "        Month (from 1 to 12).\n",
    "    \"\"\"\n",
    "    time_year, lat, lon = series.shape\n",
    "    time_month = time_year // 12\n",
    "    series_month = np.zeros((time_month, lat, lon), dtype=float)\n",
    "    series_month[::4,:,:] = series[(m-1)*4::48,:,:]\n",
    "    series_month[1::4,:,:] = series[(m-1)*4+1::48,:,:]\n",
    "    series_month[2::4,:,:] = series[(m-1)*4+2::48,:,:]\n",
    "    series_month[3::4,:,:] = series[(m-1)*4+3::48,:,:]\n",
    "    \n",
    "    return series_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with RMSE                 ########\n",
    "    ########              RMSE over time, and sum over domain                ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    RMSE_climatology, RMSE_climatology_std  = RMSE(climatology_seq * sic_max,sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_climatology = np.mean(RMSE_climatology)\n",
    "    RMSE_climatology_std = np.mean(RMSE_climatology_std)\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    RMSE_persist_0, RMSE_persist_0_std = RMSE(sic_exp_norm[-test_year*12*4-1:-1,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_persist_1, RMSE_persist_1_std = RMSE(sic_exp_norm[-test_year*12*4-1:-2,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_persist_2, RMSE_persist_2_std = RMSE(sic_exp_norm[-test_year*12*4-1:-3,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_persist_3, RMSE_persist_3_std = RMSE(sic_exp_norm[-test_year*12*4-1:-4,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_persist_4, RMSE_persist_4_std = RMSE(sic_exp_norm[-test_year*12*4-1:-5,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_persist_5, RMSE_persist_5_std = RMSE(sic_exp_norm[-test_year*12*4-1:-6,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_persist_6, RMSE_persist_6_std = RMSE(sic_exp_norm[-test_year*12*4-1:-7,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_persist_7, RMSE_persist_7_std = RMSE(sic_exp_norm[-test_year*12*4-1:-8,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_persist_8, RMSE_persist_8_std = RMSE(sic_exp_norm[-test_year*12*4-1:-9,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_persist_9, RMSE_persist_9_std = RMSE(sic_exp_norm[-test_year*12*4-1:-10,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_persist_10, RMSE_persist_10_std = RMSE(sic_exp_norm[-test_year*12*4-1:-11,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_persist_11, RMSE_persist_11_std = RMSE(sic_exp_norm[-test_year*12*4-1:-12,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_persist_12, RMSE_persist_12_std = RMSE(sic_exp_norm[-test_year*12*4-1:-13,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_persist_13, RMSE_persist_13_std = RMSE(sic_exp_norm[-test_year*12*4-1:-14,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_persist_14, RMSE_persist_14_std = RMSE(sic_exp_norm[-test_year*12*4-1:-15,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_persist_15, RMSE_persist_15_std = RMSE(sic_exp_norm[-test_year*12*4-1:-16,:,:] * sic_max,\n",
    "                                              sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print('##############################################################')\n",
    "    print('########   land-sea correction for sic prediction    #########')\n",
    "    print('##############################################################')\n",
    "    # correction for float point at 0\n",
    "    lead_pred_sic[lead_pred_sic<0] = 0\n",
    "    # extend the dimension of sea ice mask\n",
    "    sea_ice_mask_test = np.repeat(sea_ice_mask_barents[np.newaxis,:,:],test_year*48,0)\n",
    "    # correct the land cells in the prediction\n",
    "    for i in range(step_lead):\n",
    "        lead_pred_sic[:,i,:,:] = lead_pred_sic[:,i,:,:] * sea_ice_mask_test\n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    RMSE_ConvLSTM_0, RMSE_ConvLSTM_0_std = RMSE(lead_pred_sic[:,0,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_1, RMSE_ConvLSTM_1_std = RMSE(lead_pred_sic[:-1,1,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+1:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_2, RMSE_ConvLSTM_2_std = RMSE(lead_pred_sic[:-2,2,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+2:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_3, RMSE_ConvLSTM_3_std = RMSE(lead_pred_sic[:-3,3,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+3:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_4, RMSE_ConvLSTM_4_std = RMSE(lead_pred_sic[:-4,4,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+4:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_5, RMSE_ConvLSTM_5_std = RMSE(lead_pred_sic[:-5,5,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+5:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_6, RMSE_ConvLSTM_6_std = RMSE(lead_pred_sic[:-6,6,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+6:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_7, RMSE_ConvLSTM_7_std = RMSE(lead_pred_sic[:-7,7,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+7:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_8, RMSE_ConvLSTM_8_std = RMSE(lead_pred_sic[:-8,8,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+8:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_9, RMSE_ConvLSTM_9_std = RMSE(lead_pred_sic[:-9,9,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+9:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_10, RMSE_ConvLSTM_10_std = RMSE(lead_pred_sic[:-10,10,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+10:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_11, RMSE_ConvLSTM_11_std = RMSE(lead_pred_sic[:-11,11,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+11:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_12, RMSE_ConvLSTM_12_std = RMSE(lead_pred_sic[:-12,12,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+12:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_13, RMSE_ConvLSTM_13_std = RMSE(lead_pred_sic[:-13,13,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+13:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_14, RMSE_ConvLSTM_14_std = RMSE(lead_pred_sic[:-14,14,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+14:,:,:] * sic_max)\n",
    "    RMSE_ConvLSTM_15, RMSE_ConvLSTM_15_std = RMSE(lead_pred_sic[:-15,15,:,:] * sic_max,\n",
    "                                                sic_exp_norm[-test_year*12*4+15:,:,:] * sic_max)\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_0),\"+-\",np.mean(RMSE_ConvLSTM_0_std))\n",
    "    print(\"Mean RMSE with testing data - Climatology\")\n",
    "    print(RMSE_climatology,\"+-\",RMSE_climatology_std)\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_0),\"+-\",np.mean(RMSE_persist_0_std))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_1),\"+-\",np.mean(RMSE_ConvLSTM_1_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_1),\"+-\",np.mean(RMSE_persist_1_std))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_2),\"+-\",np.mean(RMSE_ConvLSTM_2_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_2),\"+-\",np.mean(RMSE_persist_2_std))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_3),\"+-\",np.mean(RMSE_ConvLSTM_3_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_3),\"+-\",np.mean(RMSE_persist_3_std))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_4),\"+-\",np.mean(RMSE_ConvLSTM_4_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_4),\"+-\",np.mean(RMSE_persist_4_std))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_5),\"+-\",np.mean(RMSE_ConvLSTM_5_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_5),\"+-\",np.mean(RMSE_persist_5_std))\n",
    "    print(\"*******************     Lead time 6     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_6),\"+-\",np.mean(RMSE_ConvLSTM_6_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_6),\"+-\",np.mean(RMSE_persist_6_std))\n",
    "    print(\"*******************     Lead time 7     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_7),\"+-\",np.mean(RMSE_ConvLSTM_7_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_7),\"+-\",np.mean(RMSE_persist_7_std))\n",
    "    print(\"*******************     Lead time 8     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_8),\"+-\",np.mean(RMSE_ConvLSTM_8_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_8),\"+-\",np.mean(RMSE_persist_8_std))\n",
    "    print(\"*******************     Lead time 9     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_9),\"+-\",np.mean(RMSE_ConvLSTM_9_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_9),\"+-\",np.mean(RMSE_persist_9_std))\n",
    "    print(\"*******************     Lead time 10     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_10),\"+-\",np.mean(RMSE_ConvLSTM_10_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_10),\"+-\",np.mean(RMSE_persist_10_std))\n",
    "    print(\"*******************     Lead time 11     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_11),\"+-\",np.mean(RMSE_ConvLSTM_11_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_11),\"+-\",np.mean(RMSE_persist_11_std))\n",
    "    print(\"*******************     Lead time 12     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_12),\"+-\",np.mean(RMSE_ConvLSTM_12_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_12),\"+-\",np.mean(RMSE_persist_12_std))\n",
    "    print(\"*******************     Lead time 13     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_13),\"+-\",np.mean(RMSE_ConvLSTM_13_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_13),\"+-\",np.mean(RMSE_persist_13_std))\n",
    "    print(\"*******************     Lead time 14     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_14),\"+-\",np.mean(RMSE_ConvLSTM_14_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_14),\"+-\",np.mean(RMSE_persist_14_std))\n",
    "    print(\"*******************     Lead time 15     *******************\")\n",
    "    print(\"Mean RMSE with testing data - ConvLSTM\")\n",
    "    print(np.mean(RMSE_ConvLSTM_15),\"+-\",np.mean(RMSE_ConvLSTM_15_std))\n",
    "    print(\"Mean RMSE with testing data - Persistence\")\n",
    "    print(np.mean(RMSE_persist_15),\"+-\",np.mean(RMSE_persist_15_std))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_rmse_pred_16weeks.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_0),np.mean(RMSE_ConvLSTM_0_std)))\n",
    "    f.write(\"RMSE - Climatology    {} + - {}\\n\".format(RMSE_climatology, RMSE_climatology_std))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_0),np.mean(RMSE_persist_0_std)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_1),np.mean(RMSE_ConvLSTM_1_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_1),np.mean(RMSE_persist_1_std)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_2),np.mean(RMSE_ConvLSTM_2_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_2),np.mean(RMSE_persist_2_std)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_3),np.mean(RMSE_ConvLSTM_3_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_3),np.mean(RMSE_persist_3_std)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_4),np.mean(RMSE_ConvLSTM_4_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_4),np.mean(RMSE_persist_4_std)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_5),np.mean(RMSE_ConvLSTM_5_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_5),np.mean(RMSE_persist_5_std)))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_6),np.mean(RMSE_ConvLSTM_6_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_6),np.mean(RMSE_persist_6_std)))\n",
    "    f.write(\"*******************     Lead time 7     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_7),np.mean(RMSE_ConvLSTM_7_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_7),np.mean(RMSE_persist_7_std)))\n",
    "    f.write(\"*******************     Lead time 8     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_8),np.mean(RMSE_ConvLSTM_8_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_8),np.mean(RMSE_persist_8_std)))\n",
    "    f.write(\"*******************     Lead time 9     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_9),np.mean(RMSE_ConvLSTM_9_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_9),np.mean(RMSE_persist_9_std)))\n",
    "    f.write(\"*******************     Lead time 10     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_10),np.mean(RMSE_ConvLSTM_10_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_10),np.mean(RMSE_persist_10_std)))\n",
    "    f.write(\"*******************     Lead time 11     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_11),np.mean(RMSE_ConvLSTM_11_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_11),np.mean(RMSE_persist_11_std)))\n",
    "    f.write(\"*******************     Lead time 12     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_12),np.mean(RMSE_ConvLSTM_12_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_12),np.mean(RMSE_persist_12_std)))\n",
    "    f.write(\"*******************     Lead time 13     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_13),np.mean(RMSE_ConvLSTM_13_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_13),np.mean(RMSE_persist_13_std)))\n",
    "    f.write(\"*******************     Lead time 14     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_14),np.mean(RMSE_ConvLSTM_14_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_14),np.mean(RMSE_persist_14_std)))\n",
    "    f.write(\"*******************     Lead time 15     *******************\\n\")\n",
    "    f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(RMSE_ConvLSTM_15),np.mean(RMSE_ConvLSTM_15_std)))\n",
    "    f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(RMSE_persist_15),np.mean(RMSE_persist_15_std)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly RMSE with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = week2month(lead_pred_sic[:,0,:,:], i)\n",
    "        persist_monthly_series = week2month(sic_exp_norm[-test_year*12*4-1:-1,:,:], i)\n",
    "        climatology_monthly_series = week2month(climatology_seq, i)\n",
    "        truth_monthly_series = week2month(sic_exp_norm[-test_year*12*4:,:,:], i)\n",
    "        \n",
    "        rmse_ConvLSTM_monthly, rmse_ConvLSTM_monthly_std = RMSE(ConvLSTM_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_persist_monthly, rmse_persist_monthly_std = RMSE(persist_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        rmse_climatology_monthly, rmse_climatology_monthly_std = RMSE(climatology_monthly_series * sic_max,truth_monthly_series * sic_max)\n",
    "        \n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"RMSE - ConvLSTM       {} + - {}\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        print(\"RMSE - Climatology    {} + - {}\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        print(\"RMSE - Persistence    {} + - {}\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"RMSE - ConvLSTM       {} + - {}\\n\".format(np.mean(rmse_ConvLSTM_monthly), np.mean(rmse_ConvLSTM_monthly_std)))\n",
    "        f.write(\"RMSE - Climatology    {} + - {}\\n\".format(np.mean(rmse_climatology_monthly), np.mean(rmse_climatology_monthly_std)))\n",
    "        f.write(\"RMSE - Persistence    {} + - {}\\n\".format(np.mean(rmse_persist_monthly), np.mean(rmse_persist_monthly_std)))\n",
    "        \n",
    "    #f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  module for calculating accuracy/recall/precision  *********************')\n",
    "    def accuracy(pred, label):\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        boolean = (pred==label)\n",
    "        accu_seq = np.mean(np.mean(boolean.astype(float),2),1)\n",
    "        accu_spa = np.mean(boolean.astype(float),0)\n",
    "        \n",
    "        return accu_seq, accu_spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########          transfer the sea ice fields into binary data           ########\n",
    "    #################################################################################\n",
    "    # ice concentration below the threshold is regarded as no ice, the value is from\n",
    "    # https://nsidc.org/cryosphere/seaice/data/terminology.html\n",
    "    criterion_0 = 0.15 \n",
    "    # remove the area weight\n",
    "    sic_exp_denorm = np.zeros(sic_exp_norm.shape, dtype=float)\n",
    "    lead_pred_sic_denorm = np.zeros(lead_pred_sic.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        lead_pred_sic_denorm[:,:,i,:] = lead_pred_sic[:,:,i,:] /dx[i+12] * dx[35]\n",
    "        sic_exp_denorm[:,i,:] = sic_exp_norm[:,i,:] / dx[i+12] * dx[35]\n",
    "    # turn sea ice fields into binary data\n",
    "    lead_pred_sic_bin = lead_pred_sic_denorm[:]\n",
    "    sic_exp_bin = sic_exp_denorm[:]\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin <= criterion_0] = 0\n",
    "    lead_pred_sic_bin[lead_pred_sic_bin > criterion_0] = 1\n",
    "    sic_exp_bin[sic_exp_bin <= criterion_0] = 0\n",
    "    sic_exp_bin[sic_exp_bin > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    lead_pred_sic_bin = lead_pred_sic_bin.astype(int)\n",
    "    sic_exp_bin = sic_exp_bin.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with accuracy               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    accu_climatology, _ = accuracy(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    accu_persist_0, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_persist_1, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_persist_2, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_persist_3, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_persist_4, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_persist_5, _ = accuracy(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    accu_ConvLSTM_0, _ = accuracy(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    accu_ConvLSTM_1, _ = accuracy(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    accu_ConvLSTM_2, _ = accuracy(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    accu_ConvLSTM_3, _ = accuracy(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    accu_ConvLSTM_4, _ = accuracy(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    accu_ConvLSTM_5, _ = accuracy(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_0))\n",
    "    print(\"Total accuracy with testing data - Climatology\")\n",
    "    print(np.mean(accu_climatology))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_1))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_2))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_3))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_4))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total accuracy with testing data - ConvLSTM\")\n",
    "    print(np.mean(accu_ConvLSTM_5))\n",
    "    print(\"Total accuracy with testing data - Persistence\")\n",
    "    print(np.mean(accu_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_accuracy_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_0)))\n",
    "    f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accu_climatology)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_1)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_2)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_3)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_4)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accu_ConvLSTM_5)))\n",
    "    f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accu_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"accuracy - ConvLSTM       {}\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Climatology    {}\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"accuracy - Persistence    {}\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"accuracy - ConvLSTM       {}\\n\".format(np.mean(accuracy(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Climatology    {}\\n\".format(np.mean(accuracy(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"accuracy - Persistence    {}\\n\".format(np.mean(accuracy(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print ('*******************  module for calculating recall/precision  *********************')\n",
    "    # positive is sea ice = 1\n",
    "    \n",
    "    def recall(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total actual positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False negative (is 1 but predict 0)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 0] = 2\n",
    "        label_dummy_2[label == 1] = 2\n",
    "        # count False Positive events\n",
    "        falseNegative = (pred_dummy_2 == label_dummy_2)\n",
    "\n",
    "#         recall_seq = np.mean(np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                     (truePositive.astype(float) + falseNegative.astype(float))),2),1)\n",
    "        \n",
    "        recall_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                       np.sum(np.sum(falseNegative.astype(float),2),1))\n",
    "        \n",
    "#         recall_spa = np.mean(np.nan_to_num(truePositive.astype(float) / \n",
    "#                                           (truePositive.astype(float) + falseNegative.astype(float))),0)\n",
    "        \n",
    "        recall_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                             np.sum(falseNegative.astype(float),0))\n",
    "        \n",
    "        #return recall_seq, recall_spa\n",
    "        return np.nan_to_num(recall_seq), np.nan_to_num(recall_spa)\n",
    "    \n",
    "    def precision(pred, label):\n",
    "        \"\"\"\n",
    "        True positive / Total predicted positive\n",
    "        Input fields must contain only 0 / 1. 1 is positive.\n",
    "        \"\"\"\n",
    "        #print(\"Input size must be [seq, lat, lon]\")\n",
    "        seq, lat, lon = pred.shape\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_1 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_1 = np.zeros(label.shape,dtype=int)\n",
    "        # True positive\n",
    "        # create dummy matrix to save the labels\n",
    "        pred_dummy_1[:] = pred[:]\n",
    "        label_dummy_1[:] = label[:]\n",
    "        # change the label of negative events\n",
    "        pred_dummy_1[pred == 0] = 2\n",
    "        label_dummy_1[label == 0] = 3\n",
    "        # count True Positive events\n",
    "        truePositive = (pred_dummy_1 == label_dummy_1)\n",
    "\n",
    "        # initialize dummy matrix\n",
    "        pred_dummy_2 = np.zeros(pred.shape,dtype=int)\n",
    "        label_dummy_2 = np.zeros(label.shape,dtype=int)\n",
    "        # False positive (is 0 but predict 1)\n",
    "        # create dummy matrix to save the labels (reset dummy)\n",
    "        pred_dummy_2[:] = pred[:]\n",
    "        label_dummy_2[:] = label[:]\n",
    "        pred_dummy_2[pred == 1] = 2\n",
    "        label_dummy_2[label == 0] = 2\n",
    "        # count False Positive events\n",
    "        falsePositive = (pred_dummy_2 == label_dummy_2)\n",
    "        \n",
    "        prec_seq = np.sum(np.sum(truePositive.astype(float),2),1) / (np.sum(np.sum(truePositive.astype(float),2),1) +\n",
    "                                                                     np.sum(np.sum(falsePositive.astype(float),2),1))\n",
    "        \n",
    "        prec_spa = np.sum(truePositive.astype(float),0) / (np.sum(truePositive.astype(float),0) +\n",
    "                                                           np.sum(falsePositive.astype(float),0))\n",
    "        \n",
    "        return np.nan_to_num(prec_seq), np.nan_to_num(prec_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########                performance evaluation with recall               ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    recall_climatology, _ = recall(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    recall_persist_0, _ = recall(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_persist_1, _ = recall(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_persist_2, _ = recall(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_persist_3, _ = recall(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_persist_4, _ = recall(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_persist_5, _ = recall(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    recall_ConvLSTM_0, _ = recall(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    recall_ConvLSTM_1, _ = recall(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    recall_ConvLSTM_2, _ = recall(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    recall_ConvLSTM_3, _ = recall(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    recall_ConvLSTM_4, _ = recall(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    recall_ConvLSTM_5, _ = recall(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_0))\n",
    "    print(\"Total recall with testing data - Climatology\")\n",
    "    print(np.mean(recall_climatology))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_1))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_2))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_3))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_4))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total recall with testing data - ConvLSTM\")\n",
    "    print(np.mean(recall_ConvLSTM_5))\n",
    "    print(\"Total recall with testing data - Persistence\")\n",
    "    print(np.mean(recall_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_recall_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total recall with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_0)))\n",
    "    f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall_climatology)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_1)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_2)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_3)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_4)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall_ConvLSTM_5)))\n",
    "    f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"recall - ConvLSTM       {}\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Climatology    {}\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"recall - Persistence    {}\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"recall - ConvLSTM       {}\\n\".format(np.mean(recall(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Climatology    {}\\n\".format(np.mean(recall(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"recall - Persistence    {}\\n\".format(np.mean(recall(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #################################################################################\n",
    "    ########              performance evaluation with precision              ########\n",
    "    #################################################################################\n",
    "    sequence_len, height, width = sic_exp_norm.shape\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with climatology  ############')\n",
    "    print('##############################################################')\n",
    "    # compute climatology\n",
    "    climatology = np.zeros((48, height, width),dtype=float)\n",
    "    for i in range(48):\n",
    "        climatology[i,:,:] = np.mean(sic_exp_norm[i::48,:,:],axis=0)\n",
    "    # repeat this climatology and calculate the RMSE\n",
    "    climatology_seq = np.tile(climatology,(test_year,1,1))\n",
    "    # turn into binary\n",
    "    climatology_denorm = np.zeros(climatology_seq.shape, dtype=float)\n",
    "    for i in np.arange(height):\n",
    "        climatology_denorm[:,i,:] = climatology_seq[:,i,:] / dx[i+12] * dx[35]    \n",
    "    climatology_denorm[climatology_denorm <= criterion_0] = 0\n",
    "    climatology_denorm[climatology_denorm > criterion_0] = 1\n",
    "    # turn matrix into int\n",
    "    climatology_denorm = climatology_denorm.astype(int)\n",
    "    \n",
    "    prec_climatology, _ = precision(climatology_denorm, sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('############   start prediction with persistence  ############')\n",
    "    print('##############################################################')\n",
    "    prec_persist_0, _ = precision(sic_exp_bin[-test_year*12*4-1:-1,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_persist_1, _ = precision(sic_exp_bin[-test_year*12*4-1:-2,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_persist_2, _ = precision(sic_exp_bin[-test_year*12*4-1:-3,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_persist_3, _ = precision(sic_exp_bin[-test_year*12*4-1:-4,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_persist_4, _ = precision(sic_exp_bin[-test_year*12*4-1:-5,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_persist_5, _ = precision(sic_exp_bin[-test_year*12*4-1:-6,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print('##############################################################')\n",
    "    print('########   start prediction with linear regression   #########')\n",
    "    print('##############################################################')\n",
    "    \n",
    "    print('#################################################################################')\n",
    "    print('############   evaluation and statistical matrix for the entire year ############')\n",
    "    print('#################################################################################')\n",
    "    prec_ConvLSTM_0, _ = precision(lead_pred_sic_bin[:,0,:,:],sic_exp_bin[-test_year*12*4:,:,:])\n",
    "    prec_ConvLSTM_1, _ = precision(lead_pred_sic_bin[:-1,1,:,:],sic_exp_bin[-test_year*12*4+1:,:,:])\n",
    "    prec_ConvLSTM_2, _ = precision(lead_pred_sic_bin[:-2,2,:,:],sic_exp_bin[-test_year*12*4+2:,:,:])\n",
    "    prec_ConvLSTM_3, _ = precision(lead_pred_sic_bin[:-3,3,:,:],sic_exp_bin[-test_year*12*4+3:,:,:])\n",
    "    prec_ConvLSTM_4, _ = precision(lead_pred_sic_bin[:-4,4,:,:],sic_exp_bin[-test_year*12*4+4:,:,:])\n",
    "    prec_ConvLSTM_5, _ = precision(lead_pred_sic_bin[:-5,5,:,:],sic_exp_bin[-test_year*12*4+5:,:,:])\n",
    "    print(\"*******************     Lead time 0     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_0))\n",
    "    print(\"Total precision with testing data - Climatology\")\n",
    "    print(np.mean(prec_climatology))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_0))\n",
    "    print(\"*******************     Lead time 1     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_1))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_1))\n",
    "    print(\"*******************     Lead time 2     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_2))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_2))\n",
    "    print(\"*******************     Lead time 3     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_3))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_3))\n",
    "    print(\"*******************     Lead time 4     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_4))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_4))\n",
    "    print(\"*******************     Lead time 5     *******************\")\n",
    "    print(\"Total precision with testing data - ConvLSTM\")\n",
    "    print(np.mean(prec_ConvLSTM_5))\n",
    "    print(\"Total precision with testing data - Persistence\")\n",
    "    print(np.mean(prec_persist_5))\n",
    "    print('##############################################################')\n",
    "    print('############           create a txt file          ############')\n",
    "    print('##############################################################')\n",
    "    f = open(os.path.join(output_path,\"report_precision_pred_bin.txt\"),\"w+\")\n",
    "    f.write(\"############   evaluation of prediction   ############\\n\")\n",
    "    f.write(\"############   Total precision with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_0)))\n",
    "    f.write(\"precision - Climatology    {}\\n\".format(np.mean(prec_climatology)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_0)))\n",
    "    f.write(\"*******************     Lead time 1     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_1)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_1)))\n",
    "    f.write(\"*******************     Lead time 2     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_2)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_2)))\n",
    "    f.write(\"*******************     Lead time 3     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_3)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_3)))\n",
    "    f.write(\"*******************     Lead time 4     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_4)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_4)))\n",
    "    f.write(\"*******************     Lead time 5     *******************\\n\")\n",
    "    f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(prec_ConvLSTM_5)))\n",
    "    f.write(\"precision - Persistence    {}\\n\".format(np.mean(prec_persist_5)))\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"-\\n\")\n",
    "    f.write(\"############   Monthly accuracy with testing data   ############\\n\")\n",
    "    f.write(\"*******************     Lead time 0     *******************\\n\")\n",
    "    for i in np.arange(1,13,1):\n",
    "        ConvLSTM_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        persist_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        climatology_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        truth_monthly_series = np.zeros((test_year*4,height, width),dtype=float)\n",
    "        for j in np.arange(4):\n",
    "            ConvLSTM_monthly_series[j::4,:,:] = lead_pred_sic_bin[(i-1)*4+j::48,0,:,:]\n",
    "            persist_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4-1+(i-1)*4+j:-1:48,:,:]\n",
    "            climatology_monthly_series[j::4,:,:] = climatology_denorm[(i-1)*4+j::48,:,:]\n",
    "            truth_monthly_series[j::4,:,:] = sic_exp_bin[-test_year*12*4+(i-1)*4+j::48,:,:]\n",
    "        print(\"*******************    {}     *******************\".format(i))\n",
    "        print(\"precision - ConvLSTM       {}\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Climatology    {}\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        print(\"precision - Persistence    {}\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"*******************    {}     *******************\\n\".format(i))\n",
    "        f.write(\"precision - ConvLSTM       {}\\n\".format(np.mean(precision(ConvLSTM_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Climatology    {}\\n\".format(np.mean(precision(climatology_monthly_series,truth_monthly_series)[0])))\n",
    "        f.write(\"precision - Persistence    {}\\n\".format(np.mean(precision(persist_monthly_series,truth_monthly_series)[0])))\n",
    "    f.write(\"*******************     Lead time 6     *******************\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
